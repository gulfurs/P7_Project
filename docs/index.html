<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>P7 Project - Unity Game</title>
    <link rel="stylesheet" href="style.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Barlow:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Vertical Navbar -->
    <nav class="sidebar">
        <a href="index.html" class="nav-link active">Home</a>
        <a href="worksheet.html" class="nav-link">Documentation</a>
        <a href="iterations.html" class="nav-link">Iterations</a>
        <a href="technical.html" class="nav-link">Technical Details</a>
    </nav>

    <div class="main">
        <!-- Header -->
        <header>
            <h1>P7 Project</h1>
            <p class="tagline">Adaptive Unity Game System</p>
        </header>

        <!-- Content Grid -->
        <div class="content">
            <!-- Left: Description -->
            <section class="description">
                <h2>Introduction</h2>
                <p>
                Can a real-time, locally deployed LLM-driven multi-agent system support natural 
                conversation in a multi-party interview setting while remaining attentive under 
                realistic latency constraints?
                </p>

                <p>
                This project explores a Unity-based interview simulation where multiple AI agents 
                generate adaptive dialogue and synchronized non-verbal behaviors such as gaze, 
                gesture timing, and attention signaling. All components—including speech recognition, 
                language generation, and speech synthesis—run locally to ensure predictable timing 
                and user privacy.
                </p>

                <p>
                The goal is to address common challenges in conversational AI, including response 
                latency and limited social awareness in multi-party dialogue, by tightly coupling 
                verbal
                </p>

                <h3>Problem Definition</h3>
                <p>
<strong>Context:</strong><br>
Despite recent advances in large language models, embodied conversational agents 
still struggle to support natural multi-party dialogue. In interview-like settings, 
AI avatars often fail to coordinate turn-taking, attention, and non-verbal behavior, 
resulting in conversations that feel delayed, socially unaware, or unnatural.
</p>

<p>
<strong>Original Problem:</strong><br>
How to design a locally deployable speech-based interaction pipeline 
(STT → LLM → TTS) that supports multi-agent conversation without breaking 
conversational grounding due to latency, turn collisions, or misaligned 
non-verbal behavior.
</p>

<p>
<strong>Research Problem:</strong><br>
Can a real-time, local LLM-driven multi-agent interview system be perceived as 
attentive and natural in a multi-party setting while operating under latency 
constraints that exceed ideal human turn-taking thresholds?
</p>

<p>
<strong>Requirements:</strong>
</p>
    <ul>
        <li>End-to-end system latency must remain within accepted HCI limits 
            (&lt; 10 seconds) and be temporally predictable</li>
        <li>Agents must coordinate turn-taking without overlapping speech</li>
        <li>Verbal responses must be synchronized with non-verbal cues 
            (gaze, gesture, attention)</li>
        <li>All components must run locally to ensure timing control and user privacy</li>
    </ul>

    <p>
    <strong>Constraints:</strong>
    </p>
    <ul>
        <li>Speech recognition accuracy vs. responsiveness trade-offs (STT)</li>
        <li>LLM generation latency and limited social reasoning in multi-party dialogue</li>
        <li>Text-to-speech expressiveness versus real-time synthesis latency</li>
        <li>Computational limits of running all models on-device</li>
    </ul>

                <h3>Results</h3>
                <p><strong>Final Outcome:</strong> [Summary of what was achieved]</p>
                <p><strong>Performance:</strong> [Key metrics]</p>

                <h3>Team</h3>
                <ul>
                <li>Simon Ingemann Axelsen</li>
                <li>Niels Christian Malte Felix Iuel</li>
                <li>Sabrina Khan</li>
                <li>Mubarik Jamall Muuse</li>
                <li>Halipha Ndukimana</li>

                </ul>    
            </section>

            <!-- Right: Gallery -->
            <section class="gallery">
                <h2>Gameplay Demo</h2>
                <div class="gallery-grid">
                    <div class="gif-item">
                        <img src="./images/gameplay-1.gif" alt="Gameplay 1">
                    </div>
                    <div class="gif-item">
                        <img src="./images/gameplay-2.gif" alt="Gameplay 2">
                    </div>
                </div>
            </section>
        </div>

        <!-- Demo Video -->
        <div class="full-width-section" style="margin-bottom: 40px;">
            <h3>Demo Video</h3>
            <div class="media-container">
                <video controls>
                    <source src="./assets/videos/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
            <p style="margin-top: 15px; color: #555;">Full demonstration of the adaptive system in action.</p>
        </div>

        <!-- Footer -->
        <footer>
            <p>View on 
                <a href="https://github.com/gulfurs/P7_Project">GitHub</a>  
                <a href="https://huggingface.co/MEDSOE">HuggingFace</a>
                <a href="#">itch.io</a>
            </p>
        </footer>
    </div>
</body>
</html>
